{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from transformers import BertConfig, BertForMaskedLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Split(\n",
    "    pattern=r\"(\\[.*?\\]]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\",\n",
    "    behavior=\"isolated\"\n",
    ")\n",
    "\n",
    "# Train the tokenizer on the MOSES dataset\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "tokenizer.train(files=[\"moses_smiles.txt\"], trainer=trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"smiles_tokenizer.json\")\n",
    "\n",
    "# Define model configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    hidden_size=256,  # Embedding dimension\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    max_position_embeddings=128,  # Adjust based on max SMILES length\n",
    ")\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # Mask 15% of tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./smiles_bert\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=1000,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,  # Preprocessed dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for a token (e.g., \"C\")\n",
    "token_id = tokenizer.convert_tokens_to_ids(\"C\")\n",
    "embedding = model.bert.embeddings.word_embeddings.weight[token_id]\n",
    "\n",
    "# Get contextual embeddings for a SMILES string\n",
    "inputs = tokenizer(\"CCO\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "last_hidden_states = outputs.hidden_states[-1]  # Shape: [batch_size, seq_len, hidden_size]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkitEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
