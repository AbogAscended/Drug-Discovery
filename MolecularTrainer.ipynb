{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T14:21:59.610408Z",
     "start_time": "2025-04-29T14:21:58.698945Z"
    }
   },
   "source": [
    "from CharRNN import CharRNN, CharRNNV2\n",
    "import torch.optim as optim, torch.nn as nn, random, bisect, torch, time, numpy as np, pandas as pd\n",
    "from onehotencoder import OneHotEncoder\n",
    "from rdkit import RDLogger, Chem\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, get_worker_info\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T14:21:59.658821Z",
     "start_time": "2025-04-29T14:21:59.657020Z"
    }
   },
   "source": [
    "#Basic one hot encoder I made to encode and decode both characters and sequences\n",
    "endecode = OneHotEncoder()\n",
    "#Hyperparameters\n",
    "vocab_size = OneHotEncoder.get_vocab_size(self = endecode)\n",
    "num_layers = 3\n",
    "n_gram = 1\n",
    "dropped_out = 0.2\n",
    "hidden_size = 1024\n",
    "learning_rate = 5e-2\n",
    "num_epochs = 40\n",
    "batch_size = 128\n",
    "temp = 1\n",
    "p = 1\n",
    "b_start = 0\n",
    "b_end = 1\n",
    "anneal_epochs = 20\n",
    "subset_fraction = 1\n",
    "sample_size = 1\n",
    "momentum = .9\n",
    "weight_decay = 1e-3"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T14:22:00.620422Z",
     "start_time": "2025-04-29T14:21:59.702217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FileDataset(Dataset):\n",
    "    def __init__(self, filepaths, encoder, n_gram):\n",
    "        self.filepaths = filepaths\n",
    "        self.encoder   = encoder\n",
    "        self.n_gram    = n_gram\n",
    "\n",
    "        # build cumulative counts & line‐offset tables as before\n",
    "        self.counts  = []\n",
    "        self.offsets = []\n",
    "        total = 0\n",
    "        for path in filepaths:\n",
    "            offs = []\n",
    "            with open(path, 'rb') as f:\n",
    "                while True:\n",
    "                    pos = f.tell()\n",
    "                    line = f.readline()\n",
    "                    if not line:\n",
    "                        break\n",
    "                    offs.append(pos)\n",
    "            total += len(offs)\n",
    "            self.counts.append(total)\n",
    "            self.offsets.append(offs)\n",
    "\n",
    "        # placeholder for per‐worker file handles\n",
    "        self.file_handles = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.counts[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ensure worker has opened files\n",
    "        if self.file_handles is None:\n",
    "            raise RuntimeError(\"file_handles not initialized – did you forget worker_init_fn?\")\n",
    "\n",
    "        # map idx → (file_idx, line_idx)\n",
    "        file_idx = bisect.bisect_right(self.counts, idx)\n",
    "        prev     = 0 if file_idx == 0 else self.counts[file_idx-1]\n",
    "        line_idx = idx - prev\n",
    "\n",
    "        # seek & read from the already-open file handle\n",
    "        fh = self.file_handles[file_idx]\n",
    "        fh.seek(self.offsets[file_idx][line_idx])\n",
    "        seq = fh.readline().decode('utf-8').strip()\n",
    "\n",
    "        # your n-gram logic\n",
    "        seq_enc = self.encoder.encode_sequence(seq)  # (L, D)\n",
    "        L, D    = seq_enc.shape\n",
    "        n       = self.n_gram\n",
    "\n",
    "        windows = [seq_enc[i : i + n]       for i in range(L - n)]\n",
    "        targets = [seq_enc[i + n].view(1, D) for i in range(L - n)]\n",
    "\n",
    "        return torch.stack(windows), torch.cat(targets, dim=0)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    Opens all files for this worker and stores them on the Dataset.\n",
    "    \"\"\"\n",
    "    worker_info = get_worker_info()\n",
    "    dataset     = worker_info.dataset  # the FileDataset instance\n",
    "    # open every file and keep the handle\n",
    "    dataset.file_handles = [\n",
    "        open(path, 'rb') for path in dataset.filepaths\n",
    "    ]\n",
    "\n",
    "class FileBatchSampler(Sampler):\n",
    "    def __init__(self, counts, batch_size, shuffle=True, drop_last=True, sample_ratio: float = 1.0):\n",
    "        self.counts     = counts\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "        self.drop_last  = drop_last\n",
    "        self.sample_ratio = sample_ratio\n",
    "\n",
    "        self.batches = []\n",
    "        prev = 0\n",
    "        for cum in counts:\n",
    "            idxs = list(range(prev, cum))\n",
    "            if shuffle:\n",
    "                random.shuffle(idxs)\n",
    "            for i in range(0, len(idxs), batch_size):\n",
    "                batch = idxs[i : i + batch_size]\n",
    "                if len(batch) == batch_size or not drop_last:\n",
    "                    self.batches.append(batch)\n",
    "            prev = cum\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(self.batches)\n",
    "        if not (0 < sample_ratio <= 1):\n",
    "            raise ValueError(\"sample_ratio must be in (0,1]\")\n",
    "        if sample_ratio < 1.0:\n",
    "            keep_n = int(len(self.batches) * sample_ratio)\n",
    "            self.batches = random.sample(self.batches, keep_n)\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "filepaths = [f\"data/seqs_len{i}.txt\" for i in list(range(26,46))]\n",
    "ds = FileDataset(filepaths, endecode, n_gram=n_gram)\n",
    "sampler = FileBatchSampler(ds.counts, batch_size=batch_size, shuffle=True ,sample_ratio=sample_size)\n",
    "loader = DataLoader(ds,\n",
    "                    batch_sampler=sampler,\n",
    "                    num_workers=10,\n",
    "                    worker_init_fn=worker_init_fn\n",
    "                    )\n",
    "# charRNN = CharRNN(vocab_size, num_layers, n_gram, dropped_out).to(device)\n",
    "charRNN = CharRNNV2(vocab_size, num_layers, n_gram, hidden_size, dropped_out).to(device)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-29T14:22:00.626570Z"
    }
   },
   "source": [
    "#Using basic cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=27)\n",
    "\n",
    "#AdamW\n",
    "optimizer = optim.AdamW(charRNN.parameters(), lr=learning_rate, amsgrad=True, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3)\n",
    "charRNN.train()\n",
    "hidden = charRNN.init_hidden(batch_size).to(device)\n",
    "#Typical training loop\n",
    "print(f'Training for {num_epochs} epochs with {len(loader)} batches of size {batch_size} and {n_gram}-gram encoding')\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    total_epoch_loss = 0.0\n",
    "    if epoch < anneal_epochs:\n",
    "        current_beta = b_start + (b_end - b_start) * (epoch / anneal_epochs)\n",
    "    else:\n",
    "        current_beta = b_end\n",
    "\n",
    "    for idx, (batch_inputs, batch_targets) in enumerate(loader):\n",
    "        print(f'Batch {idx + 1}/{len(loader)}', end='\\r')\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.squeeze(2).to(device)\n",
    "        current_batch_size = batch_inputs.size(0)\n",
    "        seq_len = batch_inputs.size(1)\n",
    "        batch_inputs = batch_inputs.view(current_batch_size, seq_len, n_gram * vocab_size).to(device)\n",
    "        target_indices = torch.argmax(batch_targets, dim=2).long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden = charRNN.init_hidden(current_batch_size).to(device)\n",
    "\n",
    "        # logits, mu, std, hidden = charRNN(batch_inputs, hidden)\n",
    "        logits, hidden = charRNN(batch_inputs, hidden)\n",
    "\n",
    "        logits_permuted = logits.permute(0, 2, 1)\n",
    "\n",
    "        reconstruction_loss = criterion(logits_permuted, target_indices)\n",
    "\n",
    "        # kl_loss = -0.5 * torch.sum(1 + torch.log(std.pow(2) + 1e-8) - mu.pow(2) - std.pow(2), dim=1)\n",
    "        # kl_loss = torch.mean(kl_loss)\n",
    "        #\n",
    "        # loss = reconstruction_loss + kl_loss * current_beta\n",
    "\n",
    "        loss = reconstruction_loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(charRNN.parameters(), 10.0)\n",
    "        optimizer.step()\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = total_epoch_loss / len(loader)\n",
    "    scheduler.step(avg_epoch_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_duration_minutes = int(epoch_duration // 60)\n",
    "    epoch_duration_seconds = int(epoch_duration % 60)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss}, Time: {epoch_duration_minutes}m {epoch_duration_seconds}s\")\n",
    "torch.save(charRNN,'Models/charRNNv1-gram.pt')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 40 epochs with 12059 batches of size 128 and 1-gram encoding\n",
      "Batch 982/12059\r"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#This is a bit wonky as its turning the output into a probability distribution and then takes the smallest group of logits to add up to the probability of top_p then samples those\n",
    "def top_p_filtering(logits_p, top_p, temp_p):\n",
    "    probs = nn.functional.softmax(logits_p.squeeze(0)[-1] / temp_p, dim=0)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=0) \n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "    sorted_indices_to_remove[0] = False\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
    "    filtered_probs = probs.masked_fill(indices_to_remove, 0).clone()\n",
    "    filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "    next_token_idx = torch.multinomial(filtered_probs, 1).item()\n",
    "    return next_token_idx\n",
    "\n",
    "def get_compound_token(s, n=n_gram):\n",
    "    if not isinstance(s, str) or not s or n <= 0:\n",
    "        return \"\"\n",
    "\n",
    "    token_parts = []\n",
    "    current_length = 0\n",
    "    string_index = 0\n",
    "\n",
    "    while current_length < n and string_index < len(s):\n",
    "        if s[string_index:].startswith('Cl'):\n",
    "            token_parts.append('Cl')\n",
    "            current_length += 1\n",
    "            string_index += 2\n",
    "        elif s[string_index:].startswith('Br'):\n",
    "            token_parts.append('Br')\n",
    "            current_length += 1\n",
    "            string_index += 2\n",
    "        else:\n",
    "            token_parts.append(s[string_index])\n",
    "            current_length += 1\n",
    "            string_index += 1\n",
    "\n",
    "    return \"\".join(token_parts)\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "def sanitize(smiles: str) -> bool:\n",
    "    \"\"\"Return True if `smiles` parses and sanitizes, False otherwise.\"\"\"\n",
    "    smi = smiles.strip()\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi, sanitize=True)\n",
    "        return mol is not None\n",
    "    except Exception as e:\n",
    "        print(f\"Error sanitizing SMILES '{smi}': {e}\")\n",
    "        return False\n",
    "\n",
    "def validate_generation(file_path: str) -> float:\n",
    "    \"\"\"Return the fraction of lines in file_path that are valid SMILES.\"\"\"\n",
    "    valid_count = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            if sanitize(line):\n",
    "                valid_count += 1\n",
    "\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return valid_count / total"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "charRNN = torch.load('Models/charRNNv1-gram.pt', weights_only=False).to(device)\n",
    "if n_gram == 1:\n",
    "    current_n_gram = endecode.encode('[BOS]').to(device)\n",
    "else:\n",
    "    string_series = pd.read_csv('data/train.csv', header=None)[0]\n",
    "    string_series = string_series[string_series.apply(lambda x: isinstance(x,str) and x !='')]\n",
    "    top_n_grams = string_series.apply(lambda s: get_compound_token(s, n=n_gram-1))\n",
    "    top_chars = (top_n_grams.value_counts()/sum(top_n_grams.value_counts())).to_dict()\n",
    "    token = np.random.choice(list(top_chars.keys()),p=list(top_chars.values()))\n",
    "    start_token = endecode.encode('[BOS]')\n",
    "    current_n_gram = endecode.encode_sequence(token,skip_append=True)\n",
    "    current_n_gram = torch.tensor(np.concatenate((start_token,current_n_gram),axis=0)).to(device)\n",
    "\n",
    "charRNN.to(device)\n",
    "charRNN.eval()\n",
    "generations = []\n",
    "for i in range(int(2e4)):\n",
    "    generation = []\n",
    "    charCount = 0\n",
    "    print(f\"Generation {i+1}/{int(2e4)}\",end='\\r')\n",
    "    with torch.no_grad():\n",
    "        hidden = charRNN.init_hidden(1).to(device)\n",
    "        while True:\n",
    "            if current_n_gram.dim() == 2:\n",
    "                current_n_gram = current_n_gram.unsqueeze(0)\n",
    "            logits,_,_, hidden = charRNN(current_n_gram, hidden)\n",
    "            next_token_index = top_p_filtering(logits, p, temp)\n",
    "            next_token = torch.zeros(vocab_size)\n",
    "            next_token[next_token_index] = 1\n",
    "            char = endecode.decode(next_token)\n",
    "            charCount += 1\n",
    "            if char == '[EOS]': break\n",
    "            generation.append(char)\n",
    "            current_n_gram = current_n_gram.squeeze(0).to(device)\n",
    "            next_token = next_token.to(device)\n",
    "            current_n_gram = torch.concat([current_n_gram[1:],next_token.unsqueeze(0)],dim=0)\n",
    "    generations.append(''.join(generation))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('data/GRUOnly1P1-gram.txt', 'w') as file:\n",
    "    for item in generations:\n",
    "        file.write(f\"{item}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Valid percentage: {validate_generation('data/GRUOnly1P1-gram.txt')}\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkitEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
