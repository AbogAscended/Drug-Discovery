{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from CharRNN import CharRNNV2\n",
    "import torch.nn as nn, random, bisect, torch, numpy as np, pandas as pd\n",
    "from onehotencoder import OneHotEncoder\n",
    "from rdkit import RDLogger, Chem\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, get_worker_info\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Basic one hot encoder I made to encode and decode both characters and sequences\n",
    "endecode = OneHotEncoder()\n",
    "#Hyperparameters\n",
    "vocab_size = OneHotEncoder.get_vocab_size(self = endecode)\n",
    "num_layers = 3\n",
    "n_gram = 1\n",
    "dropped_out = 0.2\n",
    "hidden_size = 1024\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "temp = 1\n",
    "p = 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FileDataset(Dataset):\n",
    "    def __init__(self, filepaths, encoder, n_gram):\n",
    "        self.filepaths = filepaths\n",
    "        self.encoder   = encoder\n",
    "        self.n_gram    = n_gram\n",
    "\n",
    "        # build cumulative counts & line‐offset tables as before\n",
    "        self.counts  = []\n",
    "        self.offsets = []\n",
    "        total = 0\n",
    "        for path in filepaths:\n",
    "            offs = []\n",
    "            with open(path, 'rb') as f:\n",
    "                while True:\n",
    "                    pos = f.tell()\n",
    "                    line = f.readline()\n",
    "                    if not line:\n",
    "                        break\n",
    "                    offs.append(pos)\n",
    "            total += len(offs)\n",
    "            self.counts.append(total)\n",
    "            self.offsets.append(offs)\n",
    "\n",
    "        # placeholder for per‐worker file handles\n",
    "        self.file_handles = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.counts[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ensure worker has opened files\n",
    "        if self.file_handles is None:\n",
    "            raise RuntimeError(\"file_handles not initialized – did you forget worker_init_fn?\")\n",
    "\n",
    "        # map idx → (file_idx, line_idx)\n",
    "        file_idx = bisect.bisect_right(self.counts, idx)\n",
    "        prev     = 0 if file_idx == 0 else self.counts[file_idx-1]\n",
    "        line_idx = idx - prev\n",
    "\n",
    "        # seek & read from the already-open file handle\n",
    "        fh = self.file_handles[file_idx]\n",
    "        fh.seek(self.offsets[file_idx][line_idx])\n",
    "        seq = fh.readline().decode('utf-8').strip()\n",
    "\n",
    "        # your n-gram logic\n",
    "        seq_enc = self.encoder.encode_sequence(seq)  # (L, D)\n",
    "        L, D    = seq_enc.shape\n",
    "        n       = self.n_gram\n",
    "\n",
    "        windows = [seq_enc[i : i + n]       for i in range(L - n)]\n",
    "        targets = [seq_enc[i + n].view(1, D) for i in range(L - n)]\n",
    "\n",
    "        return torch.stack(windows), torch.cat(targets, dim=0)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_info = get_worker_info()\n",
    "    dataset     = worker_info.dataset\n",
    "    dataset.file_handles = [\n",
    "        open(path, 'rb') for path in dataset.filepaths\n",
    "    ]\n",
    "\n",
    "class FileBatchSampler(Sampler):\n",
    "    def __init__(self, counts, batch_size, shuffle=True, drop_last=True, sample_ratio: float = 1.0):\n",
    "        self.counts     = counts\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "        self.drop_last  = drop_last\n",
    "        self.sample_ratio = sample_ratio\n",
    "\n",
    "        self.batches = []\n",
    "        prev = 0\n",
    "        for cum in counts:\n",
    "            idxs = list(range(prev, cum))\n",
    "            if shuffle:\n",
    "                random.shuffle(idxs)\n",
    "            for i in range(0, len(idxs), batch_size):\n",
    "                batch = idxs[i : i + batch_size]\n",
    "                if len(batch) == batch_size or not drop_last:\n",
    "                    self.batches.append(batch)\n",
    "            prev = cum\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(self.batches)\n",
    "        if not (0 < sample_ratio <= 1):\n",
    "            raise ValueError(\"sample_ratio must be in (0,1]\")\n",
    "        if sample_ratio < 1.0:\n",
    "            keep_n = int(len(self.batches) * sample_ratio)\n",
    "            self.batches = random.sample(self.batches, keep_n)\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "filepaths = [f\"data/seqs_len{i}.txt\" for i in list(range(26,46))]\n",
    "ds = FileDataset(filepaths, endecode, n_gram=n_gram)\n",
    "full_sampler = FileBatchSampler(\n",
    "    counts=ds.counts,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    sample_ratio=1.0\n",
    ")\n",
    "\n",
    "all_batches = list(full_sampler)\n",
    "random.shuffle(all_batches)\n",
    "\n",
    "val_frac  = 0.10\n",
    "n_val     = int(len(all_batches) * val_frac)\n",
    "val_batches   = all_batches[:n_val]\n",
    "train_batches = all_batches[n_val:]\n",
    "\n",
    "class ListBatchSampler(Sampler):\n",
    "    def __init__(self, batch_list):\n",
    "        self.batch_list = batch_list\n",
    "    def __iter__(self):\n",
    "        yield from self.batch_list\n",
    "    def __len__(self):\n",
    "        return len(self.batch_list)\n",
    "\n",
    "train_sampler = ListBatchSampler(train_batches)\n",
    "val_sampler   = ListBatchSampler(val_batches)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds,\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=10,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ds,\n",
    "    batch_sampler=val_sampler,\n",
    "    num_workers=10,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(0.05 * total_steps)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "charRNN = CharRNNV2(vocab_size, num_layers, n_gram, total_steps, warmup_steps, learning_rate, hidden_size, dropped_out).to(device)\n",
    "trainer = Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"cuda\",\n",
    "    precision='16-mixed',\n",
    "    accumulate_grad_batches=4,\n",
    "    logger=TensorBoardLogger(\"tb_logs\", name=\"char_rnn\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"valid_loss\", mode=\"min\"),\n",
    "        EarlyStopping(monitor=\"valid_loss\", patience=5),\n",
    "    ],\n",
    "    profiler=\"pytorch\",\n",
    ")\n",
    "trainer.fit(charRNN, train_loader, val_loader)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "torch.save(charRNN,'Models/charRNNv1-gram.pt')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "charRNN = torch.load('Models/charRNNv1-gram.pt', weights_only=False).to(device)",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkitEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
