{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from CharRNN import CharRNN\n",
    "import torch, torch.optim as optim, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from onehotencoder import OneHotEncoder\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import RDLogger, Chem\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Basic one hot encoder I made to encode and decode both characters and sequences\n",
    "endecode = OneHotEncoder()\n",
    "#Hyperparameters\n",
    "vocab_size = OneHotEncoder.get_vocab_size(self = endecode)\n",
    "num_layers = 26\n",
    "n_gram = 1\n",
    "dropped_out = 0.2\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 40\n",
    "batch_size = 64\n",
    "temp = 1\n",
    "p = .95\n",
    "b_start = 0\n",
    "b_end = 1\n",
    "anneal_epochs = 20\n",
    "subset_fraction = 0.4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "#Torch dataset because the processed inputs and outputs were over 60 gb in size\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, file_path, encoder, n_gram = 1):\n",
    "        self.n_gram = n_gram\n",
    "        self.file_path = file_path\n",
    "        self.encoder = encoder\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.lines = f.readlines()\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab and encode\n",
    "        sequence = self.lines[idx].strip()\n",
    "        seq_input  = self.encoder.encode_sequence(sequence)   # (L, D)\n",
    "        seq_target = self.encoder.encode_sequence(sequence)   # (L, D)\n",
    "        L = seq_input.size(0)\n",
    "        n = self.n_gram\n",
    "\n",
    "        # how many sliding windows actually have a real “next” char?\n",
    "        num_windows = L - (n if n > 1 else 1)\n",
    "        if num_windows <= 0:\n",
    "            # no valid windows—either skip or return empty tensors\n",
    "            return torch.empty(0, n, seq_input.size(1)), torch.empty(0, seq_input.size(1))\n",
    "\n",
    "        # build your n-gram inputs and their true next‐token targets\n",
    "        # for n=1 this is simply [x[i]] → x[i+1], for n>1 it's the sliding window\n",
    "        inputs  = [seq_input[i : i + n]     for i in range(num_windows)]\n",
    "        targets = [seq_target[i + n].unsqueeze(0) for i in range(num_windows)]\n",
    "\n",
    "        # stack into (T, n, D) and (T, D)\n",
    "        input_stack  = torch.stack(inputs)           # shape: (num_windows, n, D)\n",
    "        target_stack = torch.stack(targets).squeeze(1)  # shape: (num_windows, D)\n",
    "\n",
    "        return input_stack, target_stack\n",
    "\n",
    "#Load the dataset for working\n",
    "dataset = SequenceDataset('data/train.csv', endecode, n_gram = n_gram)\n",
    "\n",
    "full_size = len(dataset)\n",
    "subset_size = int(full_size * subset_fraction)\n",
    "all_indices = list(range(full_size))\n",
    "np.random.shuffle(all_indices)\n",
    "subset_indices = all_indices[:subset_size]\n",
    "subset_sampler = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers= 3, sampler=subset_sampler)\n",
    "charRNN = CharRNN(vocab_size, num_layers, n_gram, dropped_out).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Using basic cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=27)\n",
    "\n",
    "#AdamW\n",
    "optimizer = optim.AdamW(charRNN.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3)\n",
    "charRNN.train()\n",
    "#Typical training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    total_epoch_loss = 0.0\n",
    "    if epoch < anneal_epochs:\n",
    "        current_beta = b_start + (b_end - b_start) * (epoch / anneal_epochs)\n",
    "    else:\n",
    "        current_beta = b_end\n",
    "\n",
    "    for idx, (batch_inputs, batch_targets) in enumerate(dataloader):\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.squeeze(2).to(device)\n",
    "        current_batch_size = batch_inputs.size(0)\n",
    "        seq_len = batch_inputs.size(1)\n",
    "        batch_inputs = batch_inputs.view(current_batch_size, seq_len, n_gram * vocab_size)\n",
    "        target_indices = torch.argmax(batch_targets, dim=2).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden = charRNN.init_hidden(current_batch_size).to(device)\n",
    "\n",
    "        logits, mu, std, hidden = charRNN(batch_inputs, hidden)\n",
    "        logits_permuted = logits.permute(0, 2, 1)\n",
    "\n",
    "        reconstruction_loss = criterion(logits_permuted, target_indices)\n",
    "        kl_loss = -0.5 * torch.sum(1 + torch.log(std.pow(2) + 1e-8) - mu.pow(2) - std.pow(2), dim=1)\n",
    "        kl_loss = torch.mean(kl_loss)\n",
    "\n",
    "        loss = reconstruction_loss + kl_loss * current_beta\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = total_epoch_loss / len(dataloader)\n",
    "    scheduler.step(avg_epoch_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_duration_minutes = int(epoch_duration // 60)\n",
    "    epoch_duration_seconds = int(epoch_duration % 60)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss}, Time: {epoch_duration_minutes}m {epoch_duration_seconds}s\")\n",
    "torch.save(charRNN,'Models/charRNN1-gram.pt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#This is a bit wonky as its turning the output into a probability distribution and then takes the smallest group of logits to add up to the probability of top_p then samples those\n",
    "def top_p_filtering(logits_p, top_p, temp_p):\n",
    "    probs = nn.functional.softmax(logits_p.squeeze(0)[-1] / temp_p, dim=0)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=0) \n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "    sorted_indices_to_remove[0] = False\n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
    "    filtered_probs = probs.masked_fill(indices_to_remove, 0).clone()\n",
    "    filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "    next_token_idx = torch.multinomial(filtered_probs, 1).item()\n",
    "    return next_token_idx\n",
    "\n",
    "def get_compound_token(s, n=n_gram):\n",
    "    if not isinstance(s, str) or not s or n <= 0:\n",
    "        return \"\"\n",
    "\n",
    "    token_parts = []\n",
    "    current_length = 0\n",
    "    string_index = 0\n",
    "\n",
    "    while current_length < n and string_index < len(s):\n",
    "        if s[string_index:].startswith('Cl'):\n",
    "            token_parts.append('Cl')\n",
    "            current_length += 1\n",
    "            string_index += 2\n",
    "        elif s[string_index:].startswith('Br'):\n",
    "            token_parts.append('Br')\n",
    "            current_length += 1\n",
    "            string_index += 2\n",
    "        else:\n",
    "            token_parts.append(s[string_index])\n",
    "            current_length += 1\n",
    "            string_index += 1\n",
    "\n",
    "    return \"\".join(token_parts)\n",
    "\n",
    "def validate_generation(file_path):\n",
    "    # initialize variables\n",
    "    valid_count, invalid_count = 0, 0\n",
    "\n",
    "    # read the lines of the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        # read all lines into sequences\n",
    "        sequences = f.readlines()\n",
    "\n",
    "    # count the valid sequences and the invalid sequences\n",
    "    for sequence in sequences:\n",
    "        valid = sanitize(sequence) # validate\n",
    "        if valid == 1: # valid\n",
    "            valid_count += 1\n",
    "        else: # invalid\n",
    "            invalid_count += 1\n",
    "\n",
    "    # Get the percentage of valid VS invalid sequences\n",
    "    valid_percentage = valid_count / (valid_count + invalid_count)\n",
    "    return valid_percentage\n",
    "\n",
    "def sanitize(sequence):\n",
    "    # Disable all RDKit warnings\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "    # check sanitizing for the sequence input\n",
    "    try:\n",
    "        # attempt to sanitize\n",
    "        mol = Chem.MolFromSmiles(sequence, sanitize=True)\n",
    "        if mol:\n",
    "            return 1  # valid\n",
    "        else:\n",
    "            return 0  # invalid\n",
    "    except Exception as e:\n",
    "        print(f\"Error sanitizing molecule: {e}\")\n",
    "        return 1  # invalid with error"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "charRNN = torch.load('Models/charRNN1-gram.pt', weights_only=False).to(device)\n",
    "if n_gram == 1:\n",
    "    current_n_gram = endecode.encode('[BOS]').to(device)\n",
    "else:\n",
    "    string_series = pd.read_csv('data/train.csv', header=None)[0]\n",
    "    string_series = string_series[string_series.apply(lambda x: isinstance(x,str) and x !='')]\n",
    "    top_n_grams = string_series.apply(lambda s: get_compound_token(s, n=n_gram-1))\n",
    "    top_chars = (top_n_grams.value_counts()/sum(top_n_grams.value_counts())).to_dict()\n",
    "    token = np.random.choice(list(top_chars.keys()),p=list(top_chars.values()))\n",
    "    start_token = endecode.encode('[BOS]')\n",
    "    current_n_gram = endecode.encode_sequence(token,skip_append=True)\n",
    "    current_n_gram = torch.tensor(np.concatenate((start_token,current_n_gram),axis=0)).to(device)\n",
    "\n",
    "charRNN.to(device)\n",
    "charRNN.eval()\n",
    "generations = []\n",
    "for i in range(int(1e3)):\n",
    "    generation = []\n",
    "    charCount = 0\n",
    "    if i % 1000 == 0: print(i)\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            if current_n_gram.dim() == 2:\n",
    "                current_n_gram = current_n_gram.unsqueeze(0)\n",
    "            logits, _, _, _ = charRNN(current_n_gram)\n",
    "            next_token_index = top_p_filtering(logits, p, temp)\n",
    "            next_token = torch.zeros(vocab_size)\n",
    "            next_token[next_token_index] = 1\n",
    "            char = endecode.decode(next_token)\n",
    "            charCount += 1\n",
    "            if char == '[EOS]' or charCount >= 100: break\n",
    "            generation.append(char)\n",
    "            current_n_gram = current_n_gram.squeeze(0).to(device)\n",
    "            next_token = next_token.to(device)\n",
    "            current_n_gram = torch.concat([current_n_gram[1:],next_token.unsqueeze(0)],dim=0)\n",
    "    generations.append(''.join(generation))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('data/GRUOnly95P1-gram.txt', 'w') as file:\n",
    "    for item in generations:\n",
    "        file.write(f\"{item}\\n\")\n",
    "print(f\"Valid percentage: {validate_generation('data/GRUOnly95P1-gram.txt')}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkitEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
