{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from moses.interfaces import MosesTrainer\n",
    "from moses.utils import CharVocab, Logger\n",
    "\n",
    "\n",
    "class CharRNNTrainer(MosesTrainer):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def _train_epoch(self, model, tqdm_data, criterion, optimizer=None):\n",
    "        if optimizer is None:\n",
    "            model.eval()\n",
    "        else:\n",
    "            model.train()\n",
    "\n",
    "        postfix = {'loss': 0,\n",
    "                   'running_loss': 0}\n",
    "\n",
    "        for i, (prevs, nexts, lens) in enumerate(tqdm_data):\n",
    "            prevs = prevs.to(model.device)\n",
    "            nexts = nexts.to(model.device)\n",
    "            lens = lens.to(model.device)\n",
    "\n",
    "            outputs, _, _ = model(prevs, lens)\n",
    "\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]),\n",
    "                             nexts.view(-1))\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            postfix['loss'] = loss.item()\n",
    "            postfix['running_loss'] += (loss.item() -\n",
    "                                        postfix['running_loss']) / (i + 1)\n",
    "            tqdm_data.set_postfix(postfix)\n",
    "\n",
    "        postfix['mode'] = 'Eval' if optimizer is None else 'Train'\n",
    "        return postfix\n",
    "\n",
    "    def _train(self, model, train_loader, val_loader=None, logger=None):\n",
    "        def get_params():\n",
    "            return (p for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        device = model.device\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(get_params(), lr=self.config.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                              self.config.step_size,\n",
    "                                              self.config.gamma)\n",
    "\n",
    "        model.zero_grad()\n",
    "        for epoch in range(self.config.train_epochs):\n",
    "            scheduler.step()\n",
    "\n",
    "            tqdm_data = tqdm(train_loader,\n",
    "                             desc='Training (epoch #{})'.format(epoch))\n",
    "            postfix = self._train_epoch(model, tqdm_data, criterion, optimizer)\n",
    "            if logger is not None:\n",
    "                logger.append(postfix)\n",
    "                logger.save(self.config.log_file)\n",
    "\n",
    "            if val_loader is not None:\n",
    "                tqdm_data = tqdm(val_loader,\n",
    "                                 desc='Validation (epoch #{})'.format(epoch))\n",
    "                postfix = self._train_epoch(model, tqdm_data, criterion)\n",
    "                if logger is not None:\n",
    "                    logger.append(postfix)\n",
    "                    logger.save(self.config.log_file)\n",
    "\n",
    "            if (self.config.model_save is not None) and \\\n",
    "                    (epoch % self.config.save_frequency == 0):\n",
    "                model = model.to('cpu')\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    self.config.model_save[:-3]+'_{0:03d}.pt'.format(epoch)\n",
    "                )\n",
    "                model = model.to(device)\n",
    "\n",
    "    def get_vocabulary(self, data):\n",
    "        return CharVocab.from_data(data)\n",
    "\n",
    "    def get_collate_fn(self, model):\n",
    "        device = self.get_collate_device(model)\n",
    "\n",
    "        def collate(data):\n",
    "            data.sort(key=len, reverse=True)\n",
    "            tensors = [model.string2tensor(string, device=device)\n",
    "                       for string in data]\n",
    "\n",
    "            pad = model.vocabulary.pad\n",
    "            prevs = pad_sequence([t[:-1] for t in tensors],\n",
    "                                 batch_first=True, padding_value=pad)\n",
    "            nexts = pad_sequence([t[1:] for t in tensors],\n",
    "                                 batch_first=True, padding_value=pad)\n",
    "            lens = torch.tensor([len(t) - 1 for t in tensors],\n",
    "                                dtype=torch.long, device=device)\n",
    "            return prevs, nexts, lens\n",
    "\n",
    "        return collate\n",
    "\n",
    "    def fit(self, model, train_data, val_data=None):\n",
    "        logger = Logger() if self.config.log_file is not None else None\n",
    "\n",
    "        train_loader = self.get_dataloader(model, train_data, shuffle=True)\n",
    "        val_loader = None if val_data is None else self.get_dataloader(\n",
    "            model, val_data, shuffle=False\n",
    "        )\n",
    "\n",
    "        self._train(model, train_loader, val_loader, logger)\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
