{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:00:57.160951Z",
     "start_time": "2025-05-05T01:00:56.360785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from onehotencoder import OneHotEncoder\n",
    "from typing import List\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:00:57.173509Z",
     "start_time": "2025-05-05T01:00:57.168453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_sequence_lengths(\n",
    "    seq_filepath: str,\n",
    "    token_list: list[str],\n",
    "    max_len: int = 57,\n",
    "    allow_unknown: bool = False\n",
    ") -> dict[int, int]:\n",
    "\n",
    "    tokens_sorted = sorted(token_list, key=len, reverse=True)\n",
    "    counts = Counter()\n",
    "\n",
    "    with open(seq_filepath, 'r', encoding='utf-8') as f:\n",
    "        for lineno, line in enumerate(f, 1):\n",
    "            seq = line.rstrip('\\n')\n",
    "            i = 0\n",
    "            tokenized = []\n",
    "            while i < len(seq):\n",
    "                for tok in tokens_sorted:\n",
    "                    if seq.startswith(tok, i):\n",
    "                        tokenized.append(tok)\n",
    "                        i += len(tok)\n",
    "                        break\n",
    "                else:\n",
    "                    if allow_unknown:\n",
    "                        # emit single-char as fallback\n",
    "                        tokenized.append(seq[i])\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            f\"Unknown token at line {lineno}, position {i}: {seq[i:]!r}\"\n",
    "                        )\n",
    "\n",
    "            L = len(tokenized)\n",
    "            if 1 <= L <= max_len:\n",
    "                counts[L] += 1\n",
    "\n",
    "    return {length: counts.get(length, 0) for length in range(1, max_len + 1)}\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class TokenTrieNode:\n",
    "    __slots__ = (\"children\", \"token_end\")\n",
    "    def __init__(self):\n",
    "        self.children: Dict[str, TokenTrieNode] = {}\n",
    "        self.token_end: Optional[str] = None\n",
    "\n",
    "def build_token_trie(tokens: List[str]) -> TokenTrieNode:\n",
    "    root = TokenTrieNode()\n",
    "    for tok in tokens:\n",
    "        node = root\n",
    "        for ch in tok:\n",
    "            node = node.children.setdefault(ch, TokenTrieNode())\n",
    "        node.token_end = tok\n",
    "    return root\n",
    "\n",
    "def tokenize_sequence(seq: str, trie: TokenTrieNode, allow_unknown: bool=False) -> List[str]:\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    n = len(seq)\n",
    "    while i < n:\n",
    "        node = trie\n",
    "        last_match: Optional[str] = None\n",
    "        last_pos = i\n",
    "        j = i\n",
    "        while j < n and seq[j] in node.children:\n",
    "            node = node.children[seq[j]]\n",
    "            j += 1\n",
    "            if node.token_end:\n",
    "                last_match = node.token_end\n",
    "                last_pos = j\n",
    "        if last_match:\n",
    "            tokens.append(last_match)\n",
    "            i = last_pos\n",
    "        else:\n",
    "            if allow_unknown:\n",
    "                tokens.append(seq[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown token at pos {i} of {seq!r}\")\n",
    "    return tokens\n",
    "\n",
    "def filter_sequences_by_token_length(\n",
    "    input_path: str,\n",
    "    token_list: List[str],\n",
    "    target_len: int,\n",
    "    output_path: str,\n",
    "    allow_unknown: bool = False\n",
    ") -> None:\n",
    "\n",
    "    trie = build_token_trie(token_list)\n",
    "    processed = 0\n",
    "    matched = 0\n",
    "    start = time.time()\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            seq = line.strip()\n",
    "            if not seq:\n",
    "                continue\n",
    "            processed += 1\n",
    "            try:\n",
    "                toks = tokenize_sequence(seq, trie, allow_unknown)\n",
    "            except ValueError as e:\n",
    "                # you can log or skip; here we skip any bad lines\n",
    "                # print(f\"Skipping line {processed}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if len(toks) == target_len:\n",
    "                fout.write(seq + \"\\n\")\n",
    "                matched += 1\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(\n",
    "        f\"Processed {processed} lines in {elapsed:.2f}s, \"\n",
    "        f\"wrote {matched} sequences of token‐length {target_len} to {output_path}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:01:01.087321Z",
     "start_time": "2025-05-05T01:00:57.213615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filepath = \"data/test.csv\"\n",
    "token_list = ['Br', 'N', ')', 'c', 'o', '6', 's', 'Cl', '=', '2', ']', 'C', 'n', 'O', '4', '1', '#', 'S', 'F', '3', '[', '5', 'H', '(', '-', '[BOS]', '[EOS]', '[PAD]']\n",
    "valid_tokens = set(token_list)\n",
    "\n",
    "length_counts = count_sequence_lengths(\n",
    "        seq_filepath=filepath,\n",
    "        token_list=token_list,\n",
    "        max_len=57,\n",
    "        allow_unknown=False\n",
    "    )\n",
    "\n",
    "for length, cnt in length_counts.items():\n",
    "    print(f\"Length {length:2d}: {cnt}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length  1: 0\n",
      "Length  2: 0\n",
      "Length  3: 0\n",
      "Length  4: 0\n",
      "Length  5: 0\n",
      "Length  6: 0\n",
      "Length  7: 0\n",
      "Length  8: 0\n",
      "Length  9: 0\n",
      "Length 10: 0\n",
      "Length 11: 0\n",
      "Length 12: 0\n",
      "Length 13: 1\n",
      "Length 14: 1\n",
      "Length 15: 2\n",
      "Length 16: 9\n",
      "Length 17: 5\n",
      "Length 18: 21\n",
      "Length 19: 27\n",
      "Length 20: 70\n",
      "Length 21: 156\n",
      "Length 22: 190\n",
      "Length 23: 351\n",
      "Length 24: 555\n",
      "Length 25: 979\n",
      "Length 26: 1710\n",
      "Length 27: 2887\n",
      "Length 28: 4413\n",
      "Length 29: 5726\n",
      "Length 30: 7897\n",
      "Length 31: 9331\n",
      "Length 32: 10674\n",
      "Length 33: 11809\n",
      "Length 34: 12825\n",
      "Length 35: 13118\n",
      "Length 36: 13851\n",
      "Length 37: 14023\n",
      "Length 38: 13764\n",
      "Length 39: 12800\n",
      "Length 40: 11388\n",
      "Length 41: 9134\n",
      "Length 42: 6683\n",
      "Length 43: 4904\n",
      "Length 44: 3005\n",
      "Length 45: 1801\n",
      "Length 46: 928\n",
      "Length 47: 539\n",
      "Length 48: 274\n",
      "Length 49: 119\n",
      "Length 50: 57\n",
      "Length 51: 33\n",
      "Length 52: 10\n",
      "Length 53: 2\n",
      "Length 54: 1\n",
      "Length 55: 1\n",
      "Length 56: 0\n",
      "Length 57: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:04:23.730089Z",
     "start_time": "2025-05-05T01:01:34.299756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in (list(range(22,49))):\n",
    "    target_length = i\n",
    "    out_file = f\"data/seqs_len{i}_test.txt\"\n",
    "    filepath = \"data/train.csv\"\n",
    "    token_list = ['Br', 'N', ')', 'c', 'o', '6', 's', 'Cl', '=', '2', ']', 'C', 'n', 'O', '4', '1', '#', 'S', 'F', '3', '[', '5', 'H', '(', '-', '[BOS]', '[EOS]', '[PAD]']\n",
    "    filter_sequences_by_token_length(\n",
    "    input_path=filepath,\n",
    "    token_list=token_list,\n",
    "    target_len=target_length,\n",
    "    output_path=out_file,\n",
    "    allow_unknown=False\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1584663 lines in 6.25s, wrote 1792 sequences of token‐length 22 to data/seqs_len22_test.txt\n",
      "Processed 1584663 lines in 6.25s, wrote 3247 sequences of token‐length 23 to data/seqs_len23_test.txt\n",
      "Processed 1584663 lines in 6.20s, wrote 5108 sequences of token‐length 24 to data/seqs_len24_test.txt\n",
      "Processed 1584663 lines in 6.35s, wrote 9155 sequences of token‐length 25 to data/seqs_len25_test.txt\n",
      "Processed 1584663 lines in 6.27s, wrote 15152 sequences of token‐length 26 to data/seqs_len26_test.txt\n",
      "Processed 1584663 lines in 6.42s, wrote 25381 sequences of token‐length 27 to data/seqs_len27_test.txt\n",
      "Processed 1584663 lines in 6.23s, wrote 38700 sequences of token‐length 28 to data/seqs_len28_test.txt\n",
      "Processed 1584663 lines in 6.18s, wrote 53108 sequences of token‐length 29 to data/seqs_len29_test.txt\n",
      "Processed 1584663 lines in 6.22s, wrote 71316 sequences of token‐length 30 to data/seqs_len30_test.txt\n",
      "Processed 1584663 lines in 6.15s, wrote 84954 sequences of token‐length 31 to data/seqs_len31_test.txt\n",
      "Processed 1584663 lines in 6.35s, wrote 95481 sequences of token‐length 32 to data/seqs_len32_test.txt\n",
      "Processed 1584663 lines in 6.23s, wrote 106526 sequences of token‐length 33 to data/seqs_len33_test.txt\n",
      "Processed 1584663 lines in 6.19s, wrote 113634 sequences of token‐length 34 to data/seqs_len34_test.txt\n",
      "Processed 1584663 lines in 6.31s, wrote 120363 sequences of token‐length 35 to data/seqs_len35_test.txt\n",
      "Processed 1584663 lines in 6.45s, wrote 124303 sequences of token‐length 36 to data/seqs_len36_test.txt\n",
      "Processed 1584663 lines in 6.29s, wrote 126212 sequences of token‐length 37 to data/seqs_len37_test.txt\n",
      "Processed 1584663 lines in 6.35s, wrote 124680 sequences of token‐length 38 to data/seqs_len38_test.txt\n",
      "Processed 1584663 lines in 6.26s, wrote 113970 sequences of token‐length 39 to data/seqs_len39_test.txt\n",
      "Processed 1584663 lines in 6.42s, wrote 101709 sequences of token‐length 40 to data/seqs_len40_test.txt\n",
      "Processed 1584663 lines in 6.28s, wrote 83418 sequences of token‐length 41 to data/seqs_len41_test.txt\n",
      "Processed 1584663 lines in 6.24s, wrote 60024 sequences of token‐length 42 to data/seqs_len42_test.txt\n",
      "Processed 1584663 lines in 6.30s, wrote 43895 sequences of token‐length 43 to data/seqs_len43_test.txt\n",
      "Processed 1584663 lines in 6.33s, wrote 25990 sequences of token‐length 44 to data/seqs_len44_test.txt\n",
      "Processed 1584663 lines in 6.07s, wrote 15992 sequences of token‐length 45 to data/seqs_len45_test.txt\n",
      "Processed 1584663 lines in 6.19s, wrote 8943 sequences of token‐length 46 to data/seqs_len46_test.txt\n",
      "Processed 1584663 lines in 6.39s, wrote 4834 sequences of token‐length 47 to data/seqs_len47_test.txt\n",
      "Processed 1584663 lines in 6.26s, wrote 2511 sequences of token‐length 48 to data/seqs_len48_test.txt\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkitEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
