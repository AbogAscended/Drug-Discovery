{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T22:32:56.508893Z",
     "start_time": "2025-05-02T22:32:56.507207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from onehotencoder import OneHotEncoder\n",
    "from typing import List\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T22:32:56.524759Z",
     "start_time": "2025-05-02T22:32:56.519853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_sequence_lengths(\n",
    "    seq_filepath: str,\n",
    "    token_list: list[str],\n",
    "    max_len: int = 57,\n",
    "    allow_unknown: bool = False\n",
    ") -> dict[int, int]:\n",
    "\n",
    "    # Sort tokens by descending length so we always match the longest possible first\n",
    "    tokens_sorted = sorted(token_list, key=len, reverse=True)\n",
    "    counts = Counter()\n",
    "\n",
    "    with open(seq_filepath, 'r', encoding='utf-8') as f:\n",
    "        for lineno, line in enumerate(f, 1):\n",
    "            seq = line.rstrip('\\n')\n",
    "            i = 0\n",
    "            tokenized = []\n",
    "            while i < len(seq):\n",
    "                for tok in tokens_sorted:\n",
    "                    if seq.startswith(tok, i):\n",
    "                        tokenized.append(tok)\n",
    "                        i += len(tok)\n",
    "                        break\n",
    "                else:\n",
    "                    if allow_unknown:\n",
    "                        # emit single-char as fallback\n",
    "                        tokenized.append(seq[i])\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            f\"Unknown token at line {lineno}, position {i}: {seq[i:]!r}\"\n",
    "                        )\n",
    "\n",
    "            L = len(tokenized)\n",
    "            if 1 <= L <= max_len:\n",
    "                counts[L] += 1\n",
    "\n",
    "    # make sure every length from 1..max_len is present\n",
    "    return {length: counts.get(length, 0) for length in range(1, max_len + 1)}\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class TokenTrieNode:\n",
    "    __slots__ = (\"children\", \"token_end\")\n",
    "    def __init__(self):\n",
    "        self.children: Dict[str, TokenTrieNode] = {}\n",
    "        self.token_end: Optional[str] = None\n",
    "\n",
    "def build_token_trie(tokens: List[str]) -> TokenTrieNode:\n",
    "    root = TokenTrieNode()\n",
    "    for tok in tokens:\n",
    "        node = root\n",
    "        for ch in tok:\n",
    "            node = node.children.setdefault(ch, TokenTrieNode())\n",
    "        node.token_end = tok\n",
    "    return root\n",
    "\n",
    "def tokenize_sequence(seq: str, trie: TokenTrieNode, allow_unknown: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Greedy longest‐match tokenization using the trie.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    n = len(seq)\n",
    "    while i < n:\n",
    "        node = trie\n",
    "        last_match: Optional[str] = None\n",
    "        last_pos = i\n",
    "        j = i\n",
    "        # walk as far as possible in the trie\n",
    "        while j < n and seq[j] in node.children:\n",
    "            node = node.children[seq[j]]\n",
    "            j += 1\n",
    "            if node.token_end:\n",
    "                last_match = node.token_end\n",
    "                last_pos = j\n",
    "        if last_match:\n",
    "            tokens.append(last_match)\n",
    "            i = last_pos\n",
    "        else:\n",
    "            if allow_unknown:\n",
    "                tokens.append(seq[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                # you could also `continue` here to skip bad lines\n",
    "                raise ValueError(f\"Unknown token at pos {i} of {seq!r}\")\n",
    "    return tokens\n",
    "\n",
    "def filter_sequences_by_token_length(\n",
    "    input_path: str,\n",
    "    token_list: List[str],\n",
    "    target_len: int,\n",
    "    output_path: str,\n",
    "    allow_unknown: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads raw sequences (one per line), tokenizes each, and writes only\n",
    "    those whose token-count == target_len into output_path.\n",
    "    \"\"\"\n",
    "    trie = build_token_trie(token_list)\n",
    "    processed = 0\n",
    "    matched = 0\n",
    "    start = time.time()\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            seq = line.strip()\n",
    "            if not seq:\n",
    "                continue\n",
    "            processed += 1\n",
    "            try:\n",
    "                toks = tokenize_sequence(seq, trie, allow_unknown)\n",
    "            except ValueError as e:\n",
    "                # you can log or skip; here we skip any bad lines\n",
    "                # print(f\"Skipping line {processed}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if len(toks) == target_len:\n",
    "                fout.write(seq + \"\\n\")\n",
    "                matched += 1\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(\n",
    "        f\"Processed {processed} lines in {elapsed:.2f}s, \"\n",
    "        f\"wrote {matched} sequences of token‐length {target_len} to {output_path}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T22:33:31.293985Z",
     "start_time": "2025-05-02T22:32:56.564441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filepath = \"data/train.csv\"\n",
    "token_list = ['Br', 'N', ')', 'c', 'o', '6', 's', 'Cl', '=', '2', ']', 'C', 'n', 'O', '4', '1', '#', 'S', 'F', '3', '[', '5', 'H', '(', '-', '[BOS]', '[EOS]', '[PAD]']\n",
    "valid_tokens = set(token_list)\n",
    "\n",
    "length_counts = count_sequence_lengths(\n",
    "        seq_filepath=filepath,\n",
    "        token_list=token_list,\n",
    "        max_len=57,\n",
    "        allow_unknown=False\n",
    "    )\n",
    "\n",
    "for length, cnt in length_counts.items():\n",
    "    print(f\"Length {length:2d}: {cnt}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length  1: 0\n",
      "Length  2: 0\n",
      "Length  3: 0\n",
      "Length  4: 0\n",
      "Length  5: 0\n",
      "Length  6: 0\n",
      "Length  7: 0\n",
      "Length  8: 0\n",
      "Length  9: 0\n",
      "Length 10: 0\n",
      "Length 11: 0\n",
      "Length 12: 0\n",
      "Length 13: 21\n",
      "Length 14: 13\n",
      "Length 15: 19\n",
      "Length 16: 72\n",
      "Length 17: 63\n",
      "Length 18: 138\n",
      "Length 19: 230\n",
      "Length 20: 497\n",
      "Length 21: 1073\n",
      "Length 22: 1792\n",
      "Length 23: 3247\n",
      "Length 24: 5108\n",
      "Length 25: 9155\n",
      "Length 26: 15152\n",
      "Length 27: 25381\n",
      "Length 28: 38700\n",
      "Length 29: 53108\n",
      "Length 30: 71316\n",
      "Length 31: 84954\n",
      "Length 32: 95481\n",
      "Length 33: 106526\n",
      "Length 34: 113634\n",
      "Length 35: 120363\n",
      "Length 36: 124303\n",
      "Length 37: 126212\n",
      "Length 38: 124680\n",
      "Length 39: 113970\n",
      "Length 40: 101709\n",
      "Length 41: 83418\n",
      "Length 42: 60024\n",
      "Length 43: 43895\n",
      "Length 44: 25990\n",
      "Length 45: 15992\n",
      "Length 46: 8943\n",
      "Length 47: 4834\n",
      "Length 48: 2511\n",
      "Length 49: 1161\n",
      "Length 50: 557\n",
      "Length 51: 256\n",
      "Length 52: 101\n",
      "Length 53: 39\n",
      "Length 54: 14\n",
      "Length 55: 7\n",
      "Length 56: 2\n",
      "Length 57: 2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T22:37:32.157445Z",
     "start_time": "2025-05-02T22:34:00.697276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in (list(range(18,52))):\n",
    "    target_length = i\n",
    "    out_file = f\"data/seqs_len{i}.txt\"\n",
    "    filepath = \"data/train.csv\"\n",
    "    token_list = ['Br', 'N', ')', 'c', 'o', '6', 's', 'Cl', '=', '2', ']', 'C', 'n', 'O', '4', '1', '#', 'S', 'F', '3', '[', '5', 'H', '(', '-', '[BOS]', '[EOS]', '[PAD]']\n",
    "    filter_sequences_by_token_length(\n",
    "    input_path=filepath,\n",
    "    token_list=token_list,\n",
    "    target_len=target_length,\n",
    "    output_path=out_file,\n",
    "    allow_unknown=False\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1584663 lines in 6.21s, wrote 138 sequences of token‐length 18 to data/seqs_len18.txt\n",
      "Processed 1584663 lines in 6.28s, wrote 230 sequences of token‐length 19 to data/seqs_len19.txt\n",
      "Processed 1584663 lines in 6.23s, wrote 497 sequences of token‐length 20 to data/seqs_len20.txt\n",
      "Processed 1584663 lines in 6.20s, wrote 1073 sequences of token‐length 21 to data/seqs_len21.txt\n",
      "Processed 1584663 lines in 6.23s, wrote 1792 sequences of token‐length 22 to data/seqs_len22.txt\n",
      "Processed 1584663 lines in 6.23s, wrote 3247 sequences of token‐length 23 to data/seqs_len23.txt\n",
      "Processed 1584663 lines in 6.34s, wrote 5108 sequences of token‐length 24 to data/seqs_len24.txt\n",
      "Processed 1584663 lines in 6.20s, wrote 9155 sequences of token‐length 25 to data/seqs_len25.txt\n",
      "Processed 1584663 lines in 6.29s, wrote 15152 sequences of token‐length 26 to data/seqs_len26.txt\n",
      "Processed 1584663 lines in 6.19s, wrote 25381 sequences of token‐length 27 to data/seqs_len27.txt\n",
      "Processed 1584663 lines in 6.11s, wrote 38700 sequences of token‐length 28 to data/seqs_len28.txt\n",
      "Processed 1584663 lines in 6.17s, wrote 53108 sequences of token‐length 29 to data/seqs_len29.txt\n",
      "Processed 1584663 lines in 6.19s, wrote 71316 sequences of token‐length 30 to data/seqs_len30.txt\n",
      "Processed 1584663 lines in 6.24s, wrote 84954 sequences of token‐length 31 to data/seqs_len31.txt\n",
      "Processed 1584663 lines in 6.30s, wrote 95481 sequences of token‐length 32 to data/seqs_len32.txt\n",
      "Processed 1584663 lines in 6.24s, wrote 106526 sequences of token‐length 33 to data/seqs_len33.txt\n",
      "Processed 1584663 lines in 6.25s, wrote 113634 sequences of token‐length 34 to data/seqs_len34.txt\n",
      "Processed 1584663 lines in 6.25s, wrote 120363 sequences of token‐length 35 to data/seqs_len35.txt\n",
      "Processed 1584663 lines in 6.24s, wrote 124303 sequences of token‐length 36 to data/seqs_len36.txt\n",
      "Processed 1584663 lines in 6.20s, wrote 126212 sequences of token‐length 37 to data/seqs_len37.txt\n",
      "Processed 1584663 lines in 6.26s, wrote 124680 sequences of token‐length 38 to data/seqs_len38.txt\n",
      "Processed 1584663 lines in 6.21s, wrote 113970 sequences of token‐length 39 to data/seqs_len39.txt\n",
      "Processed 1584663 lines in 6.24s, wrote 101709 sequences of token‐length 40 to data/seqs_len40.txt\n",
      "Processed 1584663 lines in 6.12s, wrote 83418 sequences of token‐length 41 to data/seqs_len41.txt\n",
      "Processed 1584663 lines in 6.11s, wrote 60024 sequences of token‐length 42 to data/seqs_len42.txt\n",
      "Processed 1584663 lines in 6.18s, wrote 43895 sequences of token‐length 43 to data/seqs_len43.txt\n",
      "Processed 1584663 lines in 6.25s, wrote 25990 sequences of token‐length 44 to data/seqs_len44.txt\n",
      "Processed 1584663 lines in 6.31s, wrote 15992 sequences of token‐length 45 to data/seqs_len45.txt\n",
      "Processed 1584663 lines in 6.25s, wrote 8943 sequences of token‐length 46 to data/seqs_len46.txt\n",
      "Processed 1584663 lines in 6.13s, wrote 4834 sequences of token‐length 47 to data/seqs_len47.txt\n",
      "Processed 1584663 lines in 6.16s, wrote 2511 sequences of token‐length 48 to data/seqs_len48.txt\n",
      "Processed 1584663 lines in 6.27s, wrote 1161 sequences of token‐length 49 to data/seqs_len49.txt\n",
      "Processed 1584663 lines in 6.15s, wrote 557 sequences of token‐length 50 to data/seqs_len50.txt\n",
      "Processed 1584663 lines in 6.24s, wrote 256 sequences of token‐length 51 to data/seqs_len51.txt\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#AdamW\n",
    "\n",
    "\n",
    "charRNN.train()\n",
    "#Typical training loop\n",
    "print(f'Training for {num_epochs} epochs with {len(train_loader)} batches of size {batch_size}, {n_gram}-gram encoding, and {warmup_steps} lr warmup steps')\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    total_epoch_loss = 0.0\n",
    "    if epoch < anneal_epochs:\n",
    "        current_beta = b_start + (b_end - b_start) * (epoch / anneal_epochs)\n",
    "    else:\n",
    "        current_beta = b_end\n",
    "    for idx, (batch_inputs, batch_targets) in enumerate(train_loader):\n",
    "        print(f'Batch {idx + 1}/{len(train_loader)}', end='\\r')\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.squeeze(2).to(device)\n",
    "        current_batch_size = batch_inputs.size(0)\n",
    "        seq_len = batch_inputs.size(1)\n",
    "        batch_inputs = batch_inputs.view(current_batch_size, seq_len, n_gram * vocab_size).to(device)\n",
    "        target_indices = torch.argmax(batch_targets, dim=2).long().to(device)\n",
    "\n",
    "        hidden = charRNN.init_hidden(current_batch_size).to(device)\n",
    "\n",
    "        logits, hidden = charRNN(batch_inputs, hidden)\n",
    "\n",
    "        logits_permuted = logits.permute(0, 2, 1)\n",
    "\n",
    "        reconstruction_loss = criterion(logits_permuted, target_indices)\n",
    "\n",
    "        loss = reconstruction_loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(charRNN.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = total_epoch_loss / len(train_loader)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    epoch_duration_minutes = int(epoch_duration // 60)\n",
    "    epoch_duration_seconds = int(epoch_duration % 60)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_epoch_loss}, Time: {epoch_duration_minutes}m {epoch_duration_seconds}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkitEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
