{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:42:23.763968Z",
     "start_time": "2025-04-26T03:42:22.978920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from onehotencoder import OneHotEncoder\n",
    "from typing import List\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:42:24.126665Z",
     "start_time": "2025-04-26T03:42:24.120419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_sequence_lengths(\n",
    "    seq_filepath: str,\n",
    "    token_list: list[str],\n",
    "    max_len: int = 57,\n",
    "    allow_unknown: bool = False\n",
    ") -> dict[int, int]:\n",
    "\n",
    "    # Sort tokens by descending length so we always match the longest possible first\n",
    "    tokens_sorted = sorted(token_list, key=len, reverse=True)\n",
    "    counts = Counter()\n",
    "\n",
    "    with open(seq_filepath, 'r', encoding='utf-8') as f:\n",
    "        for lineno, line in enumerate(f, 1):\n",
    "            seq = line.rstrip('\\n')\n",
    "            i = 0\n",
    "            tokenized = []\n",
    "            while i < len(seq):\n",
    "                for tok in tokens_sorted:\n",
    "                    if seq.startswith(tok, i):\n",
    "                        tokenized.append(tok)\n",
    "                        i += len(tok)\n",
    "                        break\n",
    "                else:\n",
    "                    if allow_unknown:\n",
    "                        # emit single-char as fallback\n",
    "                        tokenized.append(seq[i])\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            f\"Unknown token at line {lineno}, position {i}: {seq[i:]!r}\"\n",
    "                        )\n",
    "\n",
    "            L = len(tokenized)\n",
    "            if 1 <= L <= max_len:\n",
    "                counts[L] += 1\n",
    "\n",
    "    # make sure every length from 1..max_len is present\n",
    "    return {length: counts.get(length, 0) for length in range(1, max_len + 1)}\n",
    "\n",
    "\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class TokenTrieNode:\n",
    "    __slots__ = (\"children\", \"token_end\")\n",
    "    def __init__(self):\n",
    "        self.children: Dict[str, TokenTrieNode] = {}\n",
    "        self.token_end: Optional[str] = None\n",
    "\n",
    "def build_token_trie(tokens: List[str]) -> TokenTrieNode:\n",
    "    root = TokenTrieNode()\n",
    "    for tok in tokens:\n",
    "        node = root\n",
    "        for ch in tok:\n",
    "            node = node.children.setdefault(ch, TokenTrieNode())\n",
    "        node.token_end = tok\n",
    "    return root\n",
    "\n",
    "def tokenize_sequence(seq: str, trie: TokenTrieNode, allow_unknown: bool=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Greedy longest‐match tokenization using the trie.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    n = len(seq)\n",
    "    while i < n:\n",
    "        node = trie\n",
    "        last_match: Optional[str] = None\n",
    "        last_pos = i\n",
    "        j = i\n",
    "        # walk as far as possible in the trie\n",
    "        while j < n and seq[j] in node.children:\n",
    "            node = node.children[seq[j]]\n",
    "            j += 1\n",
    "            if node.token_end:\n",
    "                last_match = node.token_end\n",
    "                last_pos = j\n",
    "        if last_match:\n",
    "            tokens.append(last_match)\n",
    "            i = last_pos\n",
    "        else:\n",
    "            if allow_unknown:\n",
    "                tokens.append(seq[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                # you could also `continue` here to skip bad lines\n",
    "                raise ValueError(f\"Unknown token at pos {i} of {seq!r}\")\n",
    "    return tokens\n",
    "\n",
    "def filter_sequences_by_token_length(\n",
    "    input_path: str,\n",
    "    token_list: List[str],\n",
    "    target_len: int,\n",
    "    output_path: str,\n",
    "    allow_unknown: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads raw sequences (one per line), tokenizes each, and writes only\n",
    "    those whose token-count == target_len into output_path.\n",
    "    \"\"\"\n",
    "    trie = build_token_trie(token_list)\n",
    "    processed = 0\n",
    "    matched = 0\n",
    "    start = time.time()\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            seq = line.strip()\n",
    "            if not seq:\n",
    "                continue\n",
    "            processed += 1\n",
    "            try:\n",
    "                toks = tokenize_sequence(seq, trie, allow_unknown)\n",
    "            except ValueError as e:\n",
    "                # you can log or skip; here we skip any bad lines\n",
    "                # print(f\"Skipping line {processed}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if len(toks) == target_len:\n",
    "                fout.write(seq + \"\\n\")\n",
    "                matched += 1\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(\n",
    "        f\"Processed {processed} lines in {elapsed:.2f}s, \"\n",
    "        f\"wrote {matched} sequences of token‐length {target_len} to {output_path}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filepath = \"data/train.csv\"\n",
    "token_list = ['Br', 'N', ')', 'c', 'o', '6', 's', 'Cl', '=', '2', ']', 'C', 'n', 'O', '4', '1', '#', 'S', 'F', '3', '[', '5', 'H', '(', '-', '[BOS]', '[EOS]', '[PAD]']\n",
    "valid_tokens = set(token_list)\n",
    "\n",
    "length_counts = count_sequence_lengths(\n",
    "        seq_filepath=filepath,\n",
    "        token_list=token_list,\n",
    "        max_len=57,\n",
    "        allow_unknown=False\n",
    "    )\n",
    "\n",
    "for length, cnt in length_counts.items():\n",
    "    print(f\"Length {length:2d}: {cnt}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T03:44:20.250285Z",
     "start_time": "2025-04-26T03:44:14.256098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_length = 26               # for example, only keep sequences of 10 tokens\n",
    "out_file = \"data/seqs_len26.txt\"\n",
    "filepath = \"data/train.csv\"\n",
    "token_list = ['Br', 'N', ')', 'c', 'o', '6', 's', 'Cl', '=', '2', ']', 'C', 'n', 'O', '4', '1', '#', 'S', 'F', '3', '[', '5', 'H', '(', '-', '[BOS]', '[EOS]', '[PAD]']\n",
    "\n",
    "filter_sequences_by_token_length(\n",
    "    input_path=filepath,\n",
    "    token_list=token_list,\n",
    "    target_len=target_length,\n",
    "    output_path=out_file,\n",
    "    allow_unknown=False\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1584663 lines in 5.99s, wrote 101709 sequences of token‐length 40 to data/seqs_len40.txt\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T04:53:52.032775Z",
     "start_time": "2025-04-26T04:51:51.141449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in (list(range(26,46))):\n",
    "    target_length = i\n",
    "    out_file = f\"data/seqs_len{i}.txt\"\n",
    "    filepath = \"data/train.csv\"\n",
    "    token_list = ['Br', 'N', ')', 'c', 'o', '6', 's', 'Cl', '=', '2', ']', 'C', 'n', 'O', '4', '1', '#', 'S', 'F', '3', '[', '5', 'H', '(', '-', '[BOS]', '[EOS]', '[PAD]']\n",
    "    filter_sequences_by_token_length(\n",
    "    input_path=filepath,\n",
    "    token_list=token_list,\n",
    "    target_len=target_length,\n",
    "    output_path=out_file,\n",
    "    allow_unknown=False\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1584663 lines in 5.93s, wrote 15152 sequences of token‐length 26 to data/seqs_len26.txt\n",
      "Processed 1584663 lines in 5.97s, wrote 25381 sequences of token‐length 27 to data/seqs_len27.txt\n",
      "Processed 1584663 lines in 5.99s, wrote 38700 sequences of token‐length 28 to data/seqs_len28.txt\n",
      "Processed 1584663 lines in 6.03s, wrote 53108 sequences of token‐length 29 to data/seqs_len29.txt\n",
      "Processed 1584663 lines in 5.96s, wrote 71316 sequences of token‐length 30 to data/seqs_len30.txt\n",
      "Processed 1584663 lines in 6.06s, wrote 84954 sequences of token‐length 31 to data/seqs_len31.txt\n",
      "Processed 1584663 lines in 6.16s, wrote 95481 sequences of token‐length 32 to data/seqs_len32.txt\n",
      "Processed 1584663 lines in 6.09s, wrote 106526 sequences of token‐length 33 to data/seqs_len33.txt\n",
      "Processed 1584663 lines in 6.08s, wrote 113634 sequences of token‐length 34 to data/seqs_len34.txt\n",
      "Processed 1584663 lines in 6.10s, wrote 120363 sequences of token‐length 35 to data/seqs_len35.txt\n",
      "Processed 1584663 lines in 6.08s, wrote 124303 sequences of token‐length 36 to data/seqs_len36.txt\n",
      "Processed 1584663 lines in 6.07s, wrote 126212 sequences of token‐length 37 to data/seqs_len37.txt\n",
      "Processed 1584663 lines in 6.01s, wrote 124680 sequences of token‐length 38 to data/seqs_len38.txt\n",
      "Processed 1584663 lines in 6.05s, wrote 113970 sequences of token‐length 39 to data/seqs_len39.txt\n",
      "Processed 1584663 lines in 6.06s, wrote 101709 sequences of token‐length 40 to data/seqs_len40.txt\n",
      "Processed 1584663 lines in 6.07s, wrote 83418 sequences of token‐length 41 to data/seqs_len41.txt\n",
      "Processed 1584663 lines in 6.00s, wrote 60024 sequences of token‐length 42 to data/seqs_len42.txt\n",
      "Processed 1584663 lines in 6.08s, wrote 43895 sequences of token‐length 43 to data/seqs_len43.txt\n",
      "Processed 1584663 lines in 6.00s, wrote 25990 sequences of token‐length 44 to data/seqs_len44.txt\n",
      "Processed 1584663 lines in 6.10s, wrote 15992 sequences of token‐length 45 to data/seqs_len45.txt\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkitEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
