{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "import unicodedata\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using device = {torch.get_default_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use \"_\" to represent an out-of-vocabulary character, that is, any character we are not handling in our model\n",
    "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
    "n_letters = len(allowed_characters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in allowed_characters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    # return our out-of-vocabulary character if we encounter a letter unknown to our model\n",
    "    if letter not in allowed_characters:\n",
    "        return allowed_characters.find(\"_\")\n",
    "    else:\n",
    "        return allowed_characters.find(letter)\n",
    "    \n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.load_time = time.localtime\n",
    "        labels_set = set()\n",
    "        self.data = []\n",
    "        self.data_tensors = []\n",
    "        self.labels = []\n",
    "        self.labels_tensors = []\n",
    "\n",
    "        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in text_files:\n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            labels_set.add(label)\n",
    "            lines = open(filename,encoding='utf-8').read().strip().split('\\n')\n",
    "            for name in lines: \n",
    "                self.data.append(name)\n",
    "                self.data_tensors.append(lineToTensor(name))\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        self.labels_uniq = list(labels_set)\n",
    "        for idx in range(len(self.labels)):\n",
    "            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
    "            self.labels_tensors.append(temp_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        data_tensor = self.data_tensors[idx]\n",
    "        label_tensor = self.labels_tensors[idx]\n",
    "\n",
    "        return label_tensor, data_tensor, data_label, data_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = NamesDataset(\"data/names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, line_tensor):\n",
    "        _, hidden = self.gru(line_tensor)\n",
    "        output = self.h2o(hidden[0])\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_from_output(output, output_labels):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    label_i = top_i[0].item()\n",
    "    return output_labels[label_i], label_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, training_data, n_epoch = 10, n_batch_size = 64, report_every = 50, learning_rate = 1e-3, criterion = nn.CrossEntropyLoss()):\n",
    "    \"\"\"\n",
    "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
    "    \"\"\"\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    rnn.train()\n",
    "    optimizer = torch.optim.AdamW(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"training on data set with n = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        rnn.zero_grad() # clear the gradients\n",
    "\n",
    "        # create some minibatches\n",
    "        # we cannot use dataloaders because each of our names is a different length\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, len(batches) //n_batch_size )\n",
    "\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch: #for each example in this batch\n",
    "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
    "                output = rnn.forward(text_tensor)\n",
    "                loss = criterion(output, label_tensor)\n",
    "                batch_loss += loss\n",
    "\n",
    "            # optimize parameters\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "\n",
    "        all_losses.append(current_loss / len(batches) )\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on data set with n = 17063\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_losses = train(rnn, train_set, n_epoch=27, report_every=5)\n",
    "end = time.time()\n",
    "print(f\"training took {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, testing_data, classes):\n",
    "    confusion = torch.zeros(len(classes), len(classes))\n",
    "\n",
    "    rnn.eval() #set to eval mode\n",
    "    with torch.no_grad(): # do not record the gradients during eval phase\n",
    "        for i in range(len(testing_data)):\n",
    "            (label_tensor, text_tensor, label, text) = testing_data[i]\n",
    "            output = rnn(text_tensor)\n",
    "            guess, guess_i = label_from_output(output, classes)\n",
    "            label_i = classes.index(label)\n",
    "            confusion[label_i][guess_i] += 1\n",
    "\n",
    "    # Normalize by dividing every row by its sum\n",
    "    for i in range(len(classes)):\n",
    "        denom = confusion[i].sum()\n",
    "        if denom > 0:\n",
    "            confusion[i] = confusion[i] / denom\n",
    "\n",
    "    # Set up plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion.cpu().numpy()) #numpy uses cpu here so we need to use a cpu version\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
    "\n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # sphinx_gallery_thumbnail_number = 2\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "evaluate(rnn, test_set, classes=alldata.labels_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
